{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch scikit-learn\n",
    "import re\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"train_enc.tsv\", sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "dev_df = pd.read_csv(\"dev_enc.tsv\", sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "dev_dataset = dev_dataset.map(tokenize_function, batched=True)from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments,DistilBertModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import EarlyStoppingCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DistilBertLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBertLSTM, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.lstm = nn.LSTM(input_size=self.distilbert.config.hidden_size, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get the output from the DistilBERT model\n",
    "        distilbert_output = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = distilbert_output.last_hidden_state\n",
    "\n",
    "        # Pass the hidden states through the LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(hidden_states)\n",
    "        last_hidden_state = h_n[-1]\n",
    "\n",
    "        # Get the logits from the fully connected layer\n",
    "        logits = self.fc(last_hidden_state)\n",
    "\n",
    "        # If labels are provided (during training), compute loss\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "# Initialize the model\n",
    "model = DistilBertLSTM()\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    print(f\"Evaluation Accuracy: {acc:.4f}\")  \n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    report_to=[\"none\"],                   \n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,\n",
    "    \n",
    ")\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.01)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()test_df = pd.read_csv(\"test_enc_unlabeled.tsv\", sep=\"\\t\", header=None, names=[\"text\"])\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_predictions = trainer.predict(test_dataset).predictions.argmax(axis=1)\n",
    "\n",
    "# Save predictions to file\n",
    "with open(\"upload_predictions.txt\", \"w\") as f:\n",
    "    for pred in test_predictions:\n",
    "        f.write(f\"{pred}\\n\")\n",
    "\n",
    "print(\"Predictions saved to upload_predictions.txt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
